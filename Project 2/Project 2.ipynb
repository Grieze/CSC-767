{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1332/1332 [00:30<00:00, 43.57it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1867/1867 [00:42<00:00, 43.62it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1387/1387 [00:34<00:00, 40.59it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1334/1334 [00:29<00:00, 45.89it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1424/1424 [00:31<00:00, 44.99it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1186/1186 [00:31<00:00, 38.25it/s]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import glob,os\n",
    "from tensorflow.keras import layers\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Using and loading the same dataset from project 1\n",
    "DATADIR = \"C:\\\\Users\\\\Aaron\\\\Documents\\\\data\\\\planes\\\\\"\n",
    "\n",
    "CATEGORIES = [\"drone\", \"fighter-jet\", \"helicopter\", \"missile\", \"passenger-plane\", \"rocket\"]\n",
    "\n",
    "img_array = []\n",
    "\n",
    "for category in CATEGORIES:  # do all categories\n",
    "    path = os.path.join(DATADIR,category)  # create path to each category\n",
    "    for img in os.listdir(path):  # iterate over each image per each category\n",
    "        img_array.append(cv2.imread(os.path.join(path,img)))  # convert to array\n",
    "        \n",
    "path = 'C:\\\\Users\\\\Aaron\\\\Documents\\\\data\\\\planes\\\\'\n",
    "files = [f for f in glob.glob(path + \"*/*.jpeg\", recursive=True)]\n",
    "\n",
    "training_data = []\n",
    "IMG_SIZE = 299\n",
    "def create_training_data():\n",
    "    for category in CATEGORIES:  # do all 6 categories\n",
    "\n",
    "        path = os.path.join(DATADIR,category)  # create path to all 6 classes\n",
    "        class_num = CATEGORIES.index(category)  # get the classification number for each class\n",
    "\n",
    "        for img in tqdm(os.listdir(path)):  # iterate over each image per category\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path,img))  # convert to array\n",
    "                data_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))  # resize to normalize data size\n",
    "                training_data.append([data_array, class_num])  # add this to our training_data\n",
    "            except Exception as e:  # in the interest in keeping the output clean...\n",
    "                pass\n",
    "            \n",
    "create_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# shuffle our training data\n",
    "random.shuffle(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "0\n",
      "0\n",
      "3\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for sample in training_data[:10]:\n",
    "    print(sample[1])\n",
    "# Just a quick peek on our shuffled data\n",
    "# The reason we do this is so that our algorithm can properly learn the images, if they were sequential the \n",
    "# the model would get easily confused and have incorrect weights and might not reach optimality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for features,label in training_data:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "\n",
    "X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 3)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "X = X/255.0\n",
    "\n",
    "# Split the data into train, valid, and test sets into 70-20-10 split , X_valid, X_test \n",
    "X_train, y_train = X[0:6000], y[0:6000]\n",
    "X_valid, y_valid = X[6000:7800], y[6000:7800]\n",
    "X_test, y_test = X[7800:8510], y[7800:8510]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for Inception-ResNet-V2\n",
    "InceptionResNetV2 = tf.keras.applications.InceptionResNetV2(\n",
    "    include_top=True,\n",
    "    weights=None,\n",
    "    input_tensor=None,\n",
    "    input_shape=None,\n",
    "    pooling=\"max\",\n",
    "    classes=6,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling Inception-ResNet-V2\n",
    "InceptionResNetV2.compile(optimizer = \"adam\", loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                          metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 1800 samples\n",
      "Epoch 1/100\n",
      " 256/6000 [>.............................] - ETA: 1:41:50 - loss: 1.7951 - accuracy: 0.1484"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-5e74c22b22d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# fiting the model and testing its performance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m InceptionResNetV2_history = InceptionResNetV2.fit(X_train, y_train, epochs = 100, batch_size = 128,\n\u001b[1;32m----> 3\u001b[1;33m                                                    verbose=1, validation_data=(X_valid, y_valid))\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\DeepLearning\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DeepLearning\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DeepLearning\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DeepLearning\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DeepLearning\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DeepLearning\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    597\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DeepLearning\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DeepLearning\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DeepLearning\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DeepLearning\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DeepLearning\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# fiting the model and testing its performance\n",
    "InceptionResNetV2_history = InceptionResNetV2.fit(X_train, y_train, epochs = 5, batch_size = 128,\n",
    "                                                   verbose=1, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for Inception-V4\n",
    "import numpy as np\n",
    "\n",
    "# Sys\n",
    "import warnings\n",
    "# Keras Core\n",
    "from keras.layers.convolutional import MaxPooling2D, Convolution2D, AveragePooling2D\n",
    "from keras.layers import Input, Dropout, Dense, Flatten, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import concatenate\n",
    "from keras import regularizers\n",
    "from keras import initializers\n",
    "from keras.models import Model\n",
    "# Backend\n",
    "from keras import backend as K\n",
    "# Utils\n",
    "from keras.utils.layer_utils import convert_all_kernels_in_model\n",
    "from keras.utils.data_utils import get_file\n",
    "\n",
    "\n",
    "#########################################################################################\n",
    "# Implements the Inception Network v4 (http://arxiv.org/pdf/1602.07261v1.pdf) in Keras. #\n",
    "#########################################################################################\n",
    "\n",
    "WEIGHTS_PATH = 'https://github.com/kentsommer/keras-inceptionV4/releases/download/2.1/inception-v4_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "WEIGHTS_PATH_NO_TOP = 'https://github.com/kentsommer/keras-inceptionV4/releases/download/2.1/inception-v4_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "\n",
    "def preprocess_input(x):\n",
    "    x = np.divide(x, 255.0)\n",
    "    x = np.subtract(x, 0.5)\n",
    "    x = np.multiply(x, 2.0)\n",
    "    return x\n",
    "\n",
    "\n",
    "def conv2d_bn(x, nb_filter, num_row, num_col,\n",
    "              padding='same', strides=(1, 1), use_bias=False):\n",
    "    \"\"\"\n",
    "    Utility function to apply conv + BN. \n",
    "    (Slightly modified from https://github.com/fchollet/keras/blob/master/keras/applications/inception_v3.py)\n",
    "    \"\"\"\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        channel_axis = 1\n",
    "    else:\n",
    "        channel_axis = -1\n",
    "    x = Convolution2D(nb_filter, (num_row, num_col),\n",
    "                      strides=strides,\n",
    "                      padding=padding,\n",
    "                      use_bias=use_bias,\n",
    "                      kernel_regularizer=regularizers.l2(0.00004),\n",
    "                      kernel_initializer=initializers.VarianceScaling(scale=2.0, mode='fan_in', distribution='normal', seed=None))(x)\n",
    "    x = BatchNormalization(axis=channel_axis, momentum=0.9997, scale=False)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def block_inception_a(input):\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        channel_axis = 1\n",
    "    else:\n",
    "        channel_axis = -1\n",
    "\n",
    "    branch_0 = conv2d_bn(input, 96, 1, 1)\n",
    "\n",
    "    branch_1 = conv2d_bn(input, 64, 1, 1)\n",
    "    branch_1 = conv2d_bn(branch_1, 96, 3, 3)\n",
    "\n",
    "    branch_2 = conv2d_bn(input, 64, 1, 1)\n",
    "    branch_2 = conv2d_bn(branch_2, 96, 3, 3)\n",
    "    branch_2 = conv2d_bn(branch_2, 96, 3, 3)\n",
    "\n",
    "    branch_3 = AveragePooling2D((3,3), strides=(1,1), padding='same')(input)\n",
    "    branch_3 = conv2d_bn(branch_3, 96, 1, 1)\n",
    "\n",
    "    x = concatenate([branch_0, branch_1, branch_2, branch_3], axis=channel_axis)\n",
    "    return x\n",
    "\n",
    "\n",
    "def block_reduction_a(input):\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        channel_axis = 1\n",
    "    else:\n",
    "        channel_axis = -1\n",
    "\n",
    "    branch_0 = conv2d_bn(input, 384, 3, 3, strides=(2,2), padding='valid')\n",
    "\n",
    "    branch_1 = conv2d_bn(input, 192, 1, 1)\n",
    "    branch_1 = conv2d_bn(branch_1, 224, 3, 3)\n",
    "    branch_1 = conv2d_bn(branch_1, 256, 3, 3, strides=(2,2), padding='valid')\n",
    "\n",
    "    branch_2 = MaxPooling2D((3,3), strides=(2,2), padding='valid')(input)\n",
    "\n",
    "    x = concatenate([branch_0, branch_1, branch_2], axis=channel_axis)\n",
    "    return x\n",
    "\n",
    "\n",
    "def block_inception_b(input):\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        channel_axis = 1\n",
    "    else:\n",
    "        channel_axis = -1\n",
    "\n",
    "    branch_0 = conv2d_bn(input, 384, 1, 1)\n",
    "\n",
    "    branch_1 = conv2d_bn(input, 192, 1, 1)\n",
    "    branch_1 = conv2d_bn(branch_1, 224, 1, 7)\n",
    "    branch_1 = conv2d_bn(branch_1, 256, 7, 1)\n",
    "\n",
    "    branch_2 = conv2d_bn(input, 192, 1, 1)\n",
    "    branch_2 = conv2d_bn(branch_2, 192, 7, 1)\n",
    "    branch_2 = conv2d_bn(branch_2, 224, 1, 7)\n",
    "    branch_2 = conv2d_bn(branch_2, 224, 7, 1)\n",
    "    branch_2 = conv2d_bn(branch_2, 256, 1, 7)\n",
    "\n",
    "    branch_3 = AveragePooling2D((3,3), strides=(1,1), padding='same')(input)\n",
    "    branch_3 = conv2d_bn(branch_3, 128, 1, 1)\n",
    "\n",
    "    x = concatenate([branch_0, branch_1, branch_2, branch_3], axis=channel_axis)\n",
    "    return x\n",
    "\n",
    "\n",
    "def block_reduction_b(input):\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        channel_axis = 1\n",
    "    else:\n",
    "        channel_axis = -1\n",
    "\n",
    "    branch_0 = conv2d_bn(input, 192, 1, 1)\n",
    "    branch_0 = conv2d_bn(branch_0, 192, 3, 3, strides=(2, 2), padding='valid')\n",
    "\n",
    "    branch_1 = conv2d_bn(input, 256, 1, 1)\n",
    "    branch_1 = conv2d_bn(branch_1, 256, 1, 7)\n",
    "    branch_1 = conv2d_bn(branch_1, 320, 7, 1)\n",
    "    branch_1 = conv2d_bn(branch_1, 320, 3, 3, strides=(2,2), padding='valid')\n",
    "\n",
    "    branch_2 = MaxPooling2D((3, 3), strides=(2, 2), padding='valid')(input)\n",
    "\n",
    "    x = concatenate([branch_0, branch_1, branch_2], axis=channel_axis)\n",
    "    return x\n",
    "\n",
    "\n",
    "def block_inception_c(input):\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        channel_axis = 1\n",
    "    else:\n",
    "        channel_axis = -1\n",
    "\n",
    "    branch_0 = conv2d_bn(input, 256, 1, 1)\n",
    "\n",
    "    branch_1 = conv2d_bn(input, 384, 1, 1)\n",
    "    branch_10 = conv2d_bn(branch_1, 256, 1, 3)\n",
    "    branch_11 = conv2d_bn(branch_1, 256, 3, 1)\n",
    "    branch_1 = concatenate([branch_10, branch_11], axis=channel_axis)\n",
    "\n",
    "\n",
    "    branch_2 = conv2d_bn(input, 384, 1, 1)\n",
    "    branch_2 = conv2d_bn(branch_2, 448, 3, 1)\n",
    "    branch_2 = conv2d_bn(branch_2, 512, 1, 3)\n",
    "    branch_20 = conv2d_bn(branch_2, 256, 1, 3)\n",
    "    branch_21 = conv2d_bn(branch_2, 256, 3, 1)\n",
    "    branch_2 = concatenate([branch_20, branch_21], axis=channel_axis)\n",
    "\n",
    "    branch_3 = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(input)\n",
    "    branch_3 = conv2d_bn(branch_3, 256, 1, 1)\n",
    "\n",
    "    x = concatenate([branch_0, branch_1, branch_2, branch_3], axis=channel_axis)\n",
    "    return x\n",
    "\n",
    "\n",
    "def inception_v4_base(input):\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        channel_axis = 1\n",
    "    else:\n",
    "        channel_axis = -1\n",
    "\n",
    "    # Input Shape is 299 x 299 x 3 (th) or 3 x 299 x 299 (th)\n",
    "    net = conv2d_bn(input, 32, 3, 3, strides=(2,2), padding='valid')\n",
    "    net = conv2d_bn(net, 32, 3, 3, padding='valid')\n",
    "    net = conv2d_bn(net, 64, 3, 3)\n",
    "\n",
    "    branch_0 = MaxPooling2D((3,3), strides=(2,2), padding='valid')(net)\n",
    "\n",
    "    branch_1 = conv2d_bn(net, 96, 3, 3, strides=(2,2), padding='valid')\n",
    "\n",
    "    net = concatenate([branch_0, branch_1], axis=channel_axis)\n",
    "\n",
    "    branch_0 = conv2d_bn(net, 64, 1, 1)\n",
    "    branch_0 = conv2d_bn(branch_0, 96, 3, 3, padding='valid')\n",
    "\n",
    "    branch_1 = conv2d_bn(net, 64, 1, 1)\n",
    "    branch_1 = conv2d_bn(branch_1, 64, 1, 7)\n",
    "    branch_1 = conv2d_bn(branch_1, 64, 7, 1)\n",
    "    branch_1 = conv2d_bn(branch_1, 96, 3, 3, padding='valid')\n",
    "\n",
    "    net = concatenate([branch_0, branch_1], axis=channel_axis)\n",
    "\n",
    "    branch_0 = conv2d_bn(net, 192, 3, 3, strides=(2,2), padding='valid')\n",
    "    branch_1 = MaxPooling2D((3,3), strides=(2,2), padding='valid')(net)\n",
    "\n",
    "    net = concatenate([branch_0, branch_1], axis=channel_axis)\n",
    "\n",
    "    # 35 x 35 x 384\n",
    "    # 4 x Inception-A blocks\n",
    "    for idx in range(4):\n",
    "    \tnet = block_inception_a(net)\n",
    "\n",
    "    # 35 x 35 x 384\n",
    "    # Reduction-A block\n",
    "    net = block_reduction_a(net)\n",
    "\n",
    "    # 17 x 17 x 1024\n",
    "    # 7 x Inception-B blocks\n",
    "    for idx in range(7):\n",
    "    \tnet = block_inception_b(net)\n",
    "\n",
    "    # 17 x 17 x 1024\n",
    "    # Reduction-B block\n",
    "    net = block_reduction_b(net)\n",
    "\n",
    "    # 8 x 8 x 1536\n",
    "    # 3 x Inception-C blocks\n",
    "    for idx in range(3):\n",
    "    \tnet = block_inception_c(net)\n",
    "\n",
    "    return net\n",
    "\n",
    "\n",
    "def inception_v4(num_classes, dropout_keep_prob, weights, include_top):\n",
    "    '''\n",
    "    Creates the inception v4 network\n",
    "    Args:\n",
    "    \tnum_classes: number of classes\n",
    "    \tdropout_keep_prob: float, the fraction to keep before final layer.\n",
    "    \n",
    "    Returns: \n",
    "    \tlogits: the logits outputs of the model.\n",
    "    '''\n",
    "\n",
    "    # Input Shape is 299 x 299 x 3 (tf) or 3 x 299 x 299 (th)\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        inputs = Input((3, 299, 299))\n",
    "    else:\n",
    "        inputs = Input((299, 299, 3))\n",
    "\n",
    "    # Make inception base\n",
    "    x = inception_v4_base(inputs)\n",
    "\n",
    "\n",
    "    # Final pooling and prediction\n",
    "    if include_top:\n",
    "        # 1 x 1 x 1536\n",
    "        x = AveragePooling2D((8,8), padding='valid')(x)\n",
    "        x = Dropout(dropout_keep_prob)(x)\n",
    "        x = Flatten()(x)\n",
    "        # 1536\n",
    "        x = Dense(units=num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs, x, name='inception_v4')\n",
    "\n",
    "    # load weights\n",
    "    if weights == 'imagenet':\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            if K.backend() == 'tensorflow':\n",
    "                warnings.warn('You are using the TensorFlow backend, yet you '\n",
    "                              'are using the Theano '\n",
    "                              'image data format convention '\n",
    "                              '(`image_data_format=\"channels_first\"`). '\n",
    "                              'For best performance, set '\n",
    "                              '`image_data_format=\"channels_last\"` in '\n",
    "                              'your Keras config '\n",
    "                              'at ~/.keras/keras.json.')\n",
    "        if include_top:\n",
    "            weights_path = get_file(\n",
    "                'inception-v4_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "                WEIGHTS_PATH,\n",
    "                cache_subdir='models',\n",
    "                md5_hash='9fe79d77f793fe874470d84ca6ba4a3b')\n",
    "        else:\n",
    "            weights_path = get_file(\n",
    "                'inception-v4_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                WEIGHTS_PATH_NO_TOP,\n",
    "                cache_subdir='models',\n",
    "                md5_hash='9296b46b5971573064d12e4669110969')\n",
    "        model.load_weights(weights_path, by_name=True)\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model(num_classes=1001, dropout_prob=0.2, weights=None, include_top=True):\n",
    "    return inception_v4(num_classes, dropout_prob, weights, include_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the actual model\n",
    "inception_v4_model = create_model(num_classes=6, dropout_prob=0.2, weights=None, include_top=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling it using adam as the optimizer\n",
    "inception_v4_model.compile(optimizer = \"adam\", loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                          metrics=['accuracy'])              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fiting the model and testing its performance\n",
    "inception_v4_model_history = inception_v4_model.fit(X_train, y_train, epochs = 100, batch_size = 128,\n",
    "                                                   verbose=1, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance of Inception_V4 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy Curves\n",
    "plt.figure(figsize=[15,10])\n",
    "plt.plot(inception_v4_model_history.history['accuracy'], 'r', linewidth=3.0)\n",
    "plt.plot(inception_v4_model_history.history['val_accuracy'],'b',linewidth=3.0)\n",
    "plt.legend(['Inception_V4 Accuracy', 'Inception_V4 Val Accuracy'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.xticks([i for i in range(0,100)],[i for i in range(1,101)])\n",
    "plt.ylabel('Accuracy vs Val_Accuracy',fontsize=16)\n",
    "plt.title('Inception_V4 Accuracy vs Val Accuracy',fontsize=16)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Curves\n",
    "plt.figure(figsize=[15,10])\n",
    "plt.plot(inception_v4_model_history.history['loss'], 'r', linewidth=3.0)\n",
    "plt.plot(inception_v4_model_history.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Inception_V4 Train Loss', 'Inception_V4 Val Loss'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.xticks([i for i in range(0,100)],[i for i in range(1,101)])\n",
    "plt.ylabel('Loss vs Val_Loss',fontsize=16)\n",
    "plt.title('Inception_V4 Loss vs Val Loss Curves',fontsize=16)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our predictions of our test set\n",
    "inception_V4_y_pred = inception_v4_model.predict(X_test)\n",
    "\n",
    "# Process the predictions for our confusion matrix by taking the max probability\n",
    "y_pred = []\n",
    "for i in range(len(inception_V4_y_pred)):\n",
    "    y_pred.append(np.argmax(inception_V4_y_pred[i]))\n",
    "    \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred) \n",
    "cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.title('Inception_V4 Confusion Matrix')\n",
    "sns.heatmap(cmn, annot=True, fmt=\".2f\", linewidths=.5, cmap=\"Blues\", yticklabels = CATEGORIES, xticklabels = CATEGORIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=CATEGORIES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "\n",
    "n_classes = len(CATEGORIES)\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(y_test)\n",
    "y_true = lb.transform(y_test)\n",
    "y_pred = lb.transform(y_pred)\n",
    "\n",
    "lw = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_roc_auc_score(y_test, y_pred, average=\"macro\"):\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test[:,i], y_pred[:,i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        \n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_pred.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    \n",
    "    # First aggregate all false positive rates\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "    # Then interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= n_classes\n",
    "\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "    # Plot all ROC curves\n",
    "    plt.figure(figsize=(21,21))\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=lw)\n",
    "\n",
    "    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=lw)\n",
    "\n",
    "    colors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal', 'red', 'green', 'brown', 'purple'])\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "            label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC and AUC Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "        \n",
    "    return roc_auc_score(y_test, y_pred, average=average)\n",
    "\n",
    "print(\"ROC_AUC_Score:\", multiclass_roc_auc_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "    \n",
    "# For each class\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "for i in range(n_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(y_true[:, i],\n",
    "                                                        y_pred[:, i])\n",
    "    average_precision[i] = average_precision_score(y_true[:, i], y_pred[:, i])\n",
    "\n",
    "# A \"micro-average\": quantifying score on all classes jointly\n",
    "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(y_true.ravel(),\n",
    "    y_pred.ravel())\n",
    "average_precision[\"micro\"] = average_precision_score(y_true, y_pred,\n",
    "                                                     average=\"micro\")\n",
    "print('Average precision score, micro-averaged over all classes: {0:0.2f}'\n",
    "      .format(average_precision[\"micro\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.step(recall['micro'], precision['micro'], color='black', alpha=1.0,\n",
    "         where='post')\n",
    "plt.fill_between(recall[\"micro\"], precision[\"micro\"], alpha=0.2, color='blue')#,\n",
    "                 #**step_kwargs)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title(\n",
    "    'Average precision score, micro-averaged over all classes: AP={0:0.2f}'\n",
    "    .format(average_precision[\"micro\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup plot details\n",
    "colors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal', 'red', 'green', 'brown', 'purple'])\n",
    "\n",
    "plt.figure(figsize=(21, 21))\n",
    "f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "lines = []\n",
    "labels = []\n",
    "for f_score in f_scores:\n",
    "    x = np.linspace(0.01, 1)\n",
    "    y = f_score * x / (2 * x - f_score)\n",
    "    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "    plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "\n",
    "lines.append(l)\n",
    "labels.append('iso-f1 curves')\n",
    "l, = plt.plot(recall[\"micro\"], precision[\"micro\"], color='gold', lw=lw)\n",
    "lines.append(l)\n",
    "labels.append('micro-average Precision-recall (area = {0:0.2f})'\n",
    "              ''.format(average_precision[\"micro\"]))\n",
    "\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    l, = plt.plot(recall[i], precision[i], color=color, lw=2)\n",
    "    lines.append(l)\n",
    "    labels.append('Precision-recall for class:{0} (area = {1:0.2f})'\n",
    "                  ''.format(i, average_precision[i]))\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.subplots_adjust(bottom=0.25)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(lines, labels, loc=(0, -.2), prop=dict(size=10))\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance of Inception_ResNet_V2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy Curves\n",
    "plt.figure(figsize=[15,10])\n",
    "plt.plot(InceptionResNetV2_history.history['accuracy'], 'r', linewidth=3.0)\n",
    "plt.plot(InceptionResNetV2_history.history['val_accuracy'],'b',linewidth=3.0)\n",
    "plt.legend(['InceptionResNetV2 Accuracy', 'InceptionResNetV2 Val Accuracy'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.xticks([i for i in range(0,100)],[i for i in range(1,101)])\n",
    "plt.ylabel('Accuracy vs Val_Accuracy',fontsize=16)\n",
    "plt.title('InceptionResNetV2 Accuracy vs Val Accuracy',fontsize=16)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Curves\n",
    "plt.figure(figsize=[15,10])\n",
    "plt.plot(InceptionResNetV2_history.history['loss'], 'r', linewidth=3.0)\n",
    "plt.plot(InceptionResNetV2_history.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['InceptionResNetV2 Train Loss', 'InceptionResNetV2 Val Loss'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.xticks([i for i in range(0,100)],[i for i in range(1,101)])\n",
    "plt.ylabel('Loss vs Val_Loss',fontsize=16)\n",
    "plt.title('InceptionResNetV2 Loss vs Val Loss Curves',fontsize=16)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our predictions of our test set\n",
    "InceptionResNetV2_y_pred = InceptionResNetV2.predict(X_test)\n",
    "\n",
    "# Process the predictions for our confusion matrix by taking the max probability\n",
    "y_pred = []\n",
    "for i in range(len(InceptionResNetV2_y_pred)):\n",
    "    y_pred.append(np.argmax(InceptionResNetV2_y_pred[i]))\n",
    "    \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred) \n",
    "cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.title('InceptionResNetV2 Confusion Matrix')\n",
    "sns.heatmap(cmn, annot=True, fmt=\".2f\", linewidths=.5, cmap=\"Blues\", yticklabels = CATEGORIES, xticklabels = CATEGORIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=CATEGORIES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "\n",
    "n_classes = len(CATEGORIES)\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(y_test)\n",
    "y_true = lb.transform(y_test)\n",
    "y_pred = lb.transform(y_pred)\n",
    "\n",
    "lw = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_roc_auc_score(y_test, y_pred, average=\"macro\"):\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test[:,i], y_pred[:,i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        \n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_pred.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    \n",
    "    # First aggregate all false positive rates\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "    # Then interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= n_classes\n",
    "\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "    # Plot all ROC curves\n",
    "    plt.figure(figsize=(21,21))\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=lw)\n",
    "\n",
    "    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=lw)\n",
    "\n",
    "    colors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal', 'red', 'green', 'brown', 'purple'])\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "            label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC and AUC Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "        \n",
    "    return roc_auc_score(y_test, y_pred, average=average)\n",
    "\n",
    "print(\"ROC_AUC_Score:\", multiclass_roc_auc_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "    \n",
    "# For each class\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "for i in range(n_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(y_true[:, i],\n",
    "                                                        y_pred[:, i])\n",
    "    average_precision[i] = average_precision_score(y_true[:, i], y_pred[:, i])\n",
    "\n",
    "# A \"micro-average\": quantifying score on all classes jointly\n",
    "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(y_true.ravel(),\n",
    "    y_pred.ravel())\n",
    "average_precision[\"micro\"] = average_precision_score(y_true, y_pred,\n",
    "                                                     average=\"micro\")\n",
    "print('Average precision score, micro-averaged over all classes: {0:0.2f}'\n",
    "      .format(average_precision[\"micro\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.step(recall['micro'], precision['micro'], color='black', alpha=1.0,\n",
    "         where='post')\n",
    "plt.fill_between(recall[\"micro\"], precision[\"micro\"], alpha=0.2, color='blue')#,\n",
    "                 #**step_kwargs)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title(\n",
    "    'Average precision score, micro-averaged over all classes: AP={0:0.2f}'\n",
    "    .format(average_precision[\"micro\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup plot details\n",
    "colors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal', 'red', 'green', 'brown', 'purple'])\n",
    "\n",
    "plt.figure(figsize=(21, 21))\n",
    "f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "lines = []\n",
    "labels = []\n",
    "for f_score in f_scores:\n",
    "    x = np.linspace(0.01, 1)\n",
    "    y = f_score * x / (2 * x - f_score)\n",
    "    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "    plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "\n",
    "lines.append(l)\n",
    "labels.append('iso-f1 curves')\n",
    "l, = plt.plot(recall[\"micro\"], precision[\"micro\"], color='gold', lw=lw)\n",
    "lines.append(l)\n",
    "labels.append('micro-average Precision-recall (area = {0:0.2f})'\n",
    "              ''.format(average_precision[\"micro\"]))\n",
    "\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    l, = plt.plot(recall[i], precision[i], color=color, lw=2)\n",
    "    lines.append(l)\n",
    "    labels.append('Precision-recall for class:{0} (area = {1:0.2f})'\n",
    "                  ''.format(i, average_precision[i]))\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.subplots_adjust(bottom=0.25)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(lines, labels, loc=(0, -.2), prop=dict(size=10))\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
